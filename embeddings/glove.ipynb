{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Embedding\n",
    "\n",
    "GloVe aims to learn word vectors such that their dot product equals the logarithm of the word co-occurrence probability.\n",
    "\n",
    "To make things cool, I will just drop the formula below:\n",
    "\n",
    "$$\n",
    "J=∑ \n",
    "i,j\n",
    "​\t\n",
    " f(X \n",
    "ij\n",
    "​\t\n",
    " )(w \n",
    "i\n",
    "T\n",
    "​\t\n",
    "  \n",
    "w\n",
    "~\n",
    "  \n",
    "j\n",
    "​\t\n",
    " +b \n",
    "i\n",
    "​\t\n",
    " + \n",
    "b\n",
    "~\n",
    "  \n",
    "j\n",
    "​\t\n",
    " −log(X \n",
    "ij\n",
    "​\t\n",
    " )) \n",
    "2\n",
    " \n",
    "$$\n",
    "\n",
    "## Implementing GloVe\n",
    "\n",
    "There are two types of GloVe implementations. Since it is based-on counting global word co-occurence statistic to determine word vector, the implementation of this method is to:\n",
    "\n",
    "1. Training the GloVe model first before use\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Initialize GloVe model]-->B[Feed forward data to GloVe];\n",
    "    B[Feed forward data to GloVe]-->C[Train the GloVe model];\n",
    "    C[Train the GloVe model]-->D[Produce dense word vectors];\n",
    "    D[Produce dense word vectors]-->E[Load GloVe embeddings];\n",
    "```\n",
    "\n",
    "2. Use Pre-Trained GloVe model.\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Load pre-trained GloVe model]-->B[Load GloVe embeddings];\n",
    "```\n",
    "\n",
    "However, since I think there are no GloVe's model for the provided dataset, we may need to take the first approach, and then for the example of the second approach will the load of the previously trained GloVe model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This part is following these methodologies below:\n",
    "\n",
    "1. Load data using Pandas\n",
    "2. Loop through `Tweet` data, get the longest sentence length\n",
    "3. If the sentence length is < 1000, then vocab length is 1000. Else, 10000\n",
    "4. Tokenize the Tweet data with Tokenizer\n",
    "5. Find Co-Occurence etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "import pandas;\n",
    "from tqdm import tqdm;\n",
    "from collections import Counter;\n",
    "import tensorflow;\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences;\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10806/10806 [00:00<00:00, 561645.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Call out step 1\n",
    "dataframe = pandas.read_csv(\"../data.csv\");\n",
    "\n",
    "# Call out step 2\n",
    "longest_characters = 0;\n",
    "vocab_length = 0;\n",
    "\n",
    "for i in tqdm(dataframe[\"Tweet\"]):\n",
    "    if len(i) > longest_characters:\n",
    "        longest_characters = len(i);\n",
    "\n",
    "# Call out step 3\n",
    "\n",
    "if(longest_characters < 1000):\n",
    "    vocab_length = 1000;\n",
    "elif(longest_characters > 1000 and longest_characters < 10000):\n",
    "    vocab_length = 10000;\n",
    "\n",
    "print(f\"Vocab Length {vocab_length}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call out step 4\n",
    "tokenizer = Tokenizer(num_words = vocab_length, oov_token = \"<OOV>\");\n",
    "tokenizer.fit_on_texts(dataframe[\"Tweet\"]);\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>435</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen  Tweet  Unnamed: 2\n",
       "0        -1    435         NaN\n",
       "1        -1      6         NaN\n",
       "2         1    292         NaN\n",
       "3         1    705         NaN\n",
       "4        -1      2         NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(dataframe[\"Tweet\"]);\n",
    "dataframe[\"Tweet\"] = pad_sequences(sequences, maxlen = 1000, padding = \"post\", truncating = \"post\");\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call out step 5\n",
    "\n",
    "# Let's say the context_size is 5 since I took chance to see the dataset and find the context after reading 5 words\n",
    "context_size = 5;\n",
    "\n",
    "# Iterate over corpus to extract context and compute co-occurrence count\n",
    "context_pairs = []\n",
    "for sequence in sequences:\n",
    "    for i, target_word_index in enumerate(sequence):\n",
    "        for j in range(max(0, i - context_size), min(len(sequence), i + context_size + 1)):\n",
    "            if j != i:\n",
    "                context_word_index = sequence[j];\n",
    "                context_pairs.append((target_word_index, context_word_index));\n",
    "\n",
    "# Compute co-occurrence count for each word pair\n",
    "co_occurrence_counts = Counter(context_pairs);\n",
    "\n",
    "# Representation of data for GloVe training\n",
    "target_words = [pair[0] for pair in context_pairs];\n",
    "context_words = [pair[1] for pair in context_pairs];\n",
    "co_occurrence_counts = [co_occurrence_counts[pair] for pair in context_pairs];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, MSELoss;\n",
    "from torch.optim import SGD;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         dot_product \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(embed_target \u001b[38;5;241m*\u001b[39m embed_context, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dot_product \u001b[38;5;241m+\u001b[39m bias_target \u001b[38;5;241m+\u001b[39m bias_context\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m GloveNN(\u001b[43mvocab_length\u001b[49m);\n\u001b[1;32m     22\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m MSELoss();\n\u001b[1;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m);\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_length' is not defined"
     ]
    }
   ],
   "source": [
    "class GloveNN(Module):\n",
    "    def __init__(self, vocab_length: int, embedding_dimension: int = 100):\n",
    "        super(GloveNN, self).__init__();\n",
    "        elf.embedding = Embedding(vocab_length, embedding_dimension);\n",
    "        self.bias_target = Embedding(vocab_length, 1);\n",
    "        self.bias_context = Embedding(vocab_length, 1);\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.bias_target.weight.data.zero_()\n",
    "        self.bias_context.weight.data.zero_()\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        embed_target = self.embedding(target)\n",
    "        embed_context = self.embedding(context)\n",
    "        bias_target = self.bias_target(target).squeeze(1)\n",
    "        bias_context = self.bias_context(context).squeeze(1)\n",
    "        dot_product = torch.sum(embed_target * embed_context, dim=1)\n",
    "        return dot_product + bias_target + bias_context\n",
    "\n",
    "model = GloveNN(vocab_length);\n",
    "loss_fn = MSELoss();\n",
    "optimizer = SGD(model.parameters(), lr = 1e-3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    total_loss = 0;\n",
    "    for target, context, co_occurrence_count in tqdm(dataframe[\"Tweet\"]):\n",
    "        optimizer.zero_grad();\n",
    "        output = model(target, context);\n",
    "        loss = loss_fn(output, co_occurrence_count);\n",
    "        loss.backward();\n",
    "        optimizer.step();\n",
    "        total_loss += loss.item();\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {total_loss}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
