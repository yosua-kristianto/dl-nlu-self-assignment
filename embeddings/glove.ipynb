{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Embedding\n",
    "\n",
    "GloVe aims to learn word vectors such that their dot product equals the logarithm of the word co-occurrence probability.\n",
    "\n",
    "To make things cool, I will just drop the formula below:\n",
    "\n",
    "$$\n",
    "J=∑ \n",
    "i,j\n",
    "​\t\n",
    " f(X \n",
    "ij\n",
    "​\t\n",
    " )(w \n",
    "i\n",
    "T\n",
    "​\t\n",
    "  \n",
    "w\n",
    "~\n",
    "  \n",
    "j\n",
    "​\t\n",
    " +b \n",
    "i\n",
    "​\t\n",
    " + \n",
    "b\n",
    "~\n",
    "  \n",
    "j\n",
    "​\t\n",
    " −log(X \n",
    "ij\n",
    "​\t\n",
    " )) \n",
    "2\n",
    " \n",
    "$$\n",
    "\n",
    "## Implementing GloVe\n",
    "\n",
    "There are two types of GloVe implementations. Since it is based-on counting global word co-occurence statistic to determine word vector, the implementation of this method is to:\n",
    "\n",
    "1. Training the GloVe model first before use\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Initialize GloVe model]-->B[Feed forward data to GloVe];\n",
    "    B[Feed forward data to GloVe]-->C[Train the GloVe model];\n",
    "    C[Train the GloVe model]-->D[Produce dense word vectors];\n",
    "    D[Produce dense word vectors]-->E[Load GloVe embeddings];\n",
    "```\n",
    "\n",
    "2. Use Pre-Trained GloVe model.\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Load pre-trained GloVe model]-->B[Load GloVe embeddings];\n",
    "```\n",
    "\n",
    "However, since I think there are no GloVe's model for the provided dataset, we may need to take the first approach, and then for the example of the second approach will the load of the previously trained GloVe model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "This part is following these methodologies below:\n",
    "\n",
    "1. Load data using Pandas\n",
    "2. Loop through `Tweet` data, get the longest sentence length\n",
    "3. If the sentence length is < 1000, then vocab length is 1000. Else, 10000\n",
    "4. Tokenize the Tweet data with Tokenizer\n",
    "5. Find Co-Occurence etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas;\n",
    "from tqdm import tqdm;\n",
    "from collections import Counter;\n",
    "import torch;\n",
    "\n",
    "import tensorflow;\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences;\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10806/10806 [00:00<00:00, 1959179.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Call out step 1\n",
    "dataframe = pandas.read_csv(\"../data.csv\");\n",
    "\n",
    "# Call out step 2\n",
    "longest_characters = 0;\n",
    "vocab_length = 0;\n",
    "\n",
    "for i in tqdm(dataframe[\"Tweet\"]):\n",
    "    if len(i) > longest_characters:\n",
    "        longest_characters = len(i);\n",
    "\n",
    "# Call out step 3\n",
    "\n",
    "if(longest_characters < 1000):\n",
    "    vocab_length = 1000;\n",
    "elif(longest_characters > 1000 and longest_characters < 10000):\n",
    "    vocab_length = 10000;\n",
    "\n",
    "print(f\"Vocab Length {vocab_length}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call out step 4\n",
    "tokenizer = Tokenizer(num_words = vocab_length, oov_token = \"<OOV>\");\n",
    "tokenizer.fit_on_texts(dataframe[\"Tweet\"]);\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimen</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>435</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentimen  Tweet  Unnamed: 2\n",
       "0        -1    435         NaN\n",
       "1        -1      6         NaN\n",
       "2         1    292         NaN\n",
       "3         1    705         NaN\n",
       "4        -1      2         NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(dataframe[\"Tweet\"]);\n",
    "dataframe[\"Tweet\"] = pad_sequences(sequences, maxlen = 1000, padding = \"post\", truncating = \"post\");\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting context and computing co-occurance count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10806/10806 [00:00<00:00, 21741.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Converting tokenized target words to tensor of int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [00:06<00:00, 182543.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tokenized contexts to tensor of int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [00:05<00:00, 212945.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing co occurance counts and converting it to tensor of float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [00:06<00:00, 192010.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Call out step 5\n",
    "\n",
    "# Let's say the context_size is 5 since I took chance to see the dataset and find the context after reading 5 words\n",
    "context_size = 5;\n",
    "\n",
    "# Iterate over corpus to extract context and compute co-occurrence count\n",
    "context_pairs = []\n",
    "print(\"Extracting context and computing co-occurance count\");\n",
    "for sequence in tqdm(sequences):\n",
    "    for i, target_word_index in enumerate(sequence):\n",
    "        for j in range(max(0, i - context_size), min(len(sequence), i + context_size + 1)):\n",
    "            if j != i:\n",
    "                context_word_index = sequence[j];\n",
    "                context_pairs.append((target_word_index, context_word_index));\n",
    "\n",
    "# Compute co-occurrence count for each word pair\n",
    "co_occurrence_counts = Counter(context_pairs);\n",
    "\n",
    "print(\"\\n\\n\");\n",
    "\n",
    "# Representation of data for GloVe training\n",
    "print(\"Converting tokenized target words to tensor of int64\");\n",
    "target_words = [torch.tensor(pair[0], dtype = torch.int64) for pair in tqdm(context_pairs)];\n",
    "\n",
    "print(\"Converting tokenized contexts to tensor of int64\");\n",
    "context_words = [torch.tensor(pair[1], dtype = torch.int64) for pair in tqdm(context_pairs)];\n",
    "\n",
    "print(\"Processing co occurance counts and converting it to tensor of float32\");\n",
    "co_occurrence_counts = [torch.tensor(co_occurrence_counts[pair], dtype = torch.float32) for pair in tqdm(context_pairs)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, MSELoss;\n",
    "from torch.optim import SGD;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveNN(Module):\n",
    "    def __init__(self, vocab_length: int, embedding_dimension: int = 100):\n",
    "        super(GloveNN, self).__init__();\n",
    "        self.embedding = Embedding(vocab_length, embedding_dimension);\n",
    "        # self.bias_target = Embedding(vocab_length, 1);\n",
    "        # self.bias_context = Embedding(vocab_length, 1);\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.5, 0.5)\n",
    "        # self.bias_target.weight.data.zero_()\n",
    "        # self.bias_context.weight.data.zero_()\n",
    "\n",
    "    def forward(self, target, context):\n",
    "        embed_target = self.embedding(target)\n",
    "        embed_context = self.embedding(context)\n",
    "        # bias_target = self.bias_target(target).squeeze(1)\n",
    "        # bias_context = self.bias_context(context).squeeze(1)\n",
    "        dot_product = torch.sum(embed_target * embed_context)\n",
    "        # return dot_product + bias_target + bias_context\n",
    "        return dot_product\n",
    "\n",
    "model = GloveNN(vocab_length);\n",
    "loss_fn = MSELoss();\n",
    "optimizer = SGD(model.parameters(), lr = 1e-3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10806, 1172736, 1172736, 1172736)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe[\"Tweet\"]), len(target_words), len(context_words), len(co_occurrence_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [10:33<00:00, 1851.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [10:17<00:00, 1899.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [10:46<00:00, 1812.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1172736/1172736 [11:00<00:00, 1776.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 532764/1172736 [04:42<05:39, 1887.36it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, co_occurrence_counts[i]);\n\u001b[0;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward();\n\u001b[1;32m----> 9\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[0;32m     10\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem();\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m);\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\torch\\optim\\sgd.py:75\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     71\u001b[0m momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m has_sparse_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[1;32m---> 75\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\torch\\optim\\sgd.py:220\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 220\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yosua\\anaconda3\\envs\\tlx-obor\\Lib\\site-packages\\torch\\optim\\sgd.py:263\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m buf\n\u001b[1;32m--> 263\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "    total_loss = 0;\n",
    "    for i in tqdm(range(len(target_words))):\n",
    "        optimizer.zero_grad();\n",
    "        output = model(target_words[i], context_words[i]);\n",
    "        loss = loss_fn(output, co_occurrence_counts[i]);\n",
    "\n",
    "        loss.backward();\n",
    "        optimizer.step();\n",
    "        total_loss += loss.item();\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {total_loss}\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
